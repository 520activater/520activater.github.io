---
title: 'KV Cache'
date: 2025-03-15
permalink: /posts/2025/0315/KV Cache/
tags:
  - LLM Inference 
  - KV Cache
  - Attention Mechanism
---

# 1 文本生成的初始化与解码阶段
## 1.1 **Logit 是什么？**

- 在 Transformer 解码器的最后，我们不会直接输出文本，而是输出一个 **logit 向量**。
- **Logit** 是神经网络输出的**未归一化得分**，对应于模型词汇表（vocabulary）中的每个单词的可能性。
- 这个 **logit 向量的长度等于词汇表的大小**（例如 50,000 个单词的词汇表，logit 也有 50,000 个数值）。
- 这些 **logit 数值需要经过 Softmax 变换**，转换成概率分布。

**示例：** 假设词汇表包含 `["apple", "banana", "cat", "dog"]`，某个时间步的解码器输出 logit 可能是：
logits = [2.3, -1.2, 0.5, 1.8]
然后，经过 Softmax，我们得到：
probabilities = [0.45, 0.02, 0.12, 0.41]

## 1.2 词元
**词元（Token）**是模型的基本处理单位，通常是**一个单词**或者**子词（subword）**
- - 在英文中，"Transformer" 可能是一个 token。
    - 在中文中，"机器学习" 可能会被分成 "机器" 和 "学习" 两个 token。
    - 对于 BPE（Byte Pair Encoding）等子词切分方式，"unhappiness" 可能会被分成 ["un", "happiness"]。
- **词元搜索策略（token search strategy）**用于将 logit 转换成具体的 token，比如：
    
    - **贪心搜索（Greedy Search）**：每次选取概率最大的词。
    - **束搜索（Beam Search）**：考虑多个候选词，提高整体句子质量。
    - **采样（Sampling）**：引入随机性，使生成的文本更加多样化。


Logit与词元关系
- **Decoder 计算 logits**
- **通过 Softmax 得到词汇表上的概率**
- **使用解码策略（如 Greedy Search）选择最可能的 token**
- **输出 token**（例如 "578" 对应某个单词）


## 1.3 解码策略（Decoding Strategy）

**解码策略**是指 **如何从 Transformer 解码器输出的 logits 选出最终的 token（词元）**，即**如何决定下一步输出什么**。不同的解码策略会影响生成文本的质量、多样性和计算复杂度。
解码策略决定了 **从 logits 到 token 的转换方式**

## 1.4 推理引擎（Inference Engine）
**推理引擎** 是指 **执行 LLM 推理任务的底层计算框架**，它涉及到计算优化、存储管理、并行计算等技术，目的是让 LLM 在**尽可能短的时间内完成推理**。
- **加载模型权重**
    - 负责管理 LLM 的参数存储（可能在 CPU、GPU、TPU、专用 AI 硬件上）。
- **执行前向传播（Forward Pass）**
    - 计算 Transformer 解码器的输出 logits（即词汇表的未归一化概率）。
- **缓存和优化**
    - 使用 **KV 缓存（Key-Value Cache）** 加速解码过程，避免重复计算。
    - 采用 **张量并行（Tensor Parallelism）**、**流水线并行（Pipeline Parallelism）** 来加速大模型推理。
- **管理解码策略**
    - **推理引擎不会直接决定如何解码，但它需要执行解码策略**。
    - 例如，PyTorch、TensorRT、vLLM 这些推理引擎可以提供 **快速实现 Greedy Search、Beam Search、Top-k 采样等方法**。
- **分布式推理**
    - 如果模型太大，推理引擎需要**跨多块 GPU/TPU 进行分布式推理**。
    - 例如 GPT-4 这样的超大模型，通常需要多个 GPU 节点协同运行。

解码策略与推理引擎
解码策略是推理引擎的一部分，推理引擎负责执行解码策略

## 1.5 分词（Tokenization）
**分词（Tokenization）** 是指 **将输入的文本（prompt）转换为一组 token（词元）**，以便神经网络可以处理。
在 Transformer 模型（如 GPT-4）中，神经网络不能直接处理文本，而是需要将文本转换成**数字化的 token 序列**。


从一个输入文本序列（通常称为**提示（prompt）**）中生成文本（通常名为**完成（completion）**）通常包含步骤1-7

1-7是初始化阶段（initialization）
1. 将模型权重加载到GPU
**2-4是分词的步骤**
2. 输入的文本：What color is the sky?
3. 经过分词（Tokenization):["What", "color", "is", "the", "sky", "?"]
4. 每个 token 被映射成一个 **唯一的整数 ID**（基于词汇表）：[3923, 1933, 374, 279, 13180, 30]
这个**整数序列（token ID）**才是 LLM 模型的实际输入

分词后的 token（整数序列）会先在 CPU 上生成，然后传输到 GPU 进行推理计算。
5. CPU 生成 token 张量：例如，将 `[3923, 1933, 374, 279, 13180, 30]` 转换为 PyTorch Tensor
```
import torch
tokens = torch.tensor([3923, 1933, 374, 279, 13180, 30], dtype=torch.long)
```
6. 传输到 GPU:将 token 张量转移到 GPU 进行神经网络计算：
```
tokens = tokens.to("cuda")
```
7. GPU 进行 Transformer 计算：LLM 模型在 GPU 上执行推理，计算下一个 token。**LLM 在 GPU 上运行前向传播，基于已有的 token 计算输出**。
8. 将生成的新token（新词元）添加到输入词元序列，然后将其用作新的输入，以生成下一个词元。然后，重复这一过程，直到生成停止序列（例如单个序列结束（EOS）词元），或达到预先配置的最大序列长度（见下图）。
	这一多步骤**阶段**通常被称为**生成阶段**、**_解码阶段_**、**自回归阶段**甚至是**增量阶段**(incremental phase)
![图片1](https://520activater.github.io/images/blog-posts/KV-Cache1.png)


9. 将最终输出的词元传输到CPU，并进行逆词元化（detokenization）以获取生成的文本（见下图）
![图片2](https://520activater.github.io/images/blog-posts/KV-Cache2.png)

| 对比项         | CPU                          | GPU                                      |
|--------------|-----------------------------|-----------------------------------------|
| 并行计算能力  | 低（单线程/多线程）         | 高（成千上万的CUDA核心并行计算）       |
| 矩阵运算速度  | 慢（优化较少）              | 快（专门优化深度学习计算）             |
| 适用于        | 分词、预处理、控制逻辑       | 神经网络推理（前向传播）、深度学习计算 |


# 2 KV缓存（Cache）
## 2.1 主要挑战
self-attention layer与length of sequence（提示词元和生成补全词元）的计算成本呈二次方扩展。
 **计算过程：自注意力的 Q-K 计算**
在 Transformer 的自注意力（Self-Attention）中，每个输入 token 都会生成：
- **查询（Query, Q）**
- **键（Key, K）**
- **值（Value, V）**
计算注意力得分的公式是：$$Attention = softmax(\frac{QK^T}{\sqrt(d_k)}V)$$ <br>
Q维度：$$(n,d)$$   n是序列长度，d是隐藏层维度(即词元的嵌入向量维度)
K维度： $$(n,d)$$
故需要执行一个$$n×d$$乘$$d×n$$ 的矩阵乘法，得到一个$$n×n$$的矩阵，其中d一般是个固定的，故七计算复杂度为$$O(n^2)$$
而且涉及到softmax对$$n×n$$的矩阵进行normalization处理，因此也需要$$O(n^2)$$的计算复杂度。

KV缓存，是LLM推理过程中的一种常用优化方式，使得（自）注意力机制的计算需求在总序列长度（提示词元和生成词元）上线性扩展，而不是呈二次方扩展。

KV缓存通过在生成过程中计算出的键和值张量存储（“缓存”）在GPU内存中，从而在每个生成步骤中节省了对过去词元键和值张量的重新计算。可以理解为**以内存换计算**。

## 2.2 注意力计算的二次方扩展
名词解释：**FLOPs（Floating Point Operations per Second）** 是 **每秒浮点运算次数** 的缩写，用于衡量计算设备（如 CPU、GPU、TPU）的计算能力。它表示在一秒钟内可以执行多少次浮点运算（如加法、乘法、除法等）。
### 2.2.1 矩阵乘法的 FLOPs 计算
假设我们有两个矩阵：
- $$A$$ 形状为 **$$(n, p)$$**，即 $$n$$ 行 $$p$$ 列。
- $$B$$ 形状为 **$$(p, m)$$**，即 $$p$$ 行 $$m$$ 列。
矩阵乘法的结果是：
- **$$C = A \times B$$**，其形状为 **$$(n, m)$$**。
对于每个元素 $$C[i, j]$$，计算过程是：
$$
C[i, j] = \sum_{k=1}^{p} A[i, k] \times B[k, j]
$$
这意味着 **每个元素计算需要 $$p$$ 次乘法和 $$p-1$$ 次加法**。  
为了简化 FLOPs 计算，我们认为 **1 次乘法 + 1 次加法 ≈ 2 FLOPs**。
$$
\text{总 FLOPs} \approx 2 \times n \times m \times p
$$
---
### 2.2.2 注意力分数计算的 FLOPs
在 **自注意力（self-attention）** 机制中，我们需要计算**注意力分数**：
$$
A = QK^T
$$
其中：
- $$Q$$（查询矩阵，**query**）：形状 $$(t, d_{\text{head}})$$  
- $$K$$（键矩阵，**key**）：形状 $$(t, d_{\text{head}})$$
计算 $$A = QK^T$$ 时：
- $$Q$$ 形状 **$$(t, d_{\text{head}})$$**，
- $$K^T$$ 形状 **$$(d_{\text{head}}, t)$$**，
- 结果 $$A$$ 形状为 **$$(t, t)$$**（即 $$t \times t$$ 的注意力矩阵）。

根据矩阵乘法 FLOPs 计算：
$$
\text{FLOPs} = 2 \times d_{\text{head}} \times t^2
$$
---
### 2.2.3 输入特征的线性变换

查询 $$Q$$、键 $$K$$、值 $$V$$ 是由输入 $$X$$ 通过 **权重矩阵** 映射得到：

$$
Q = XW_Q, \quad K = XW_K, \quad V = XW_V
$$
设输入 $X$ 形状为 $(t, d_{\text{model}})$，权重矩阵 $W_Q, W_K, W_V$ 形状为 $(d_{\text{model}}, d_{\text{head}})$。
计算量为：
$$
2 \cdot t \cdot d_{\text{model}} \cdot d_{\text{head}}
$$
**计算复杂度**: $O(t)$。

### 2.2.4 前馈神经网络 (FFN)
Transformer 每个层都有一个 **前馈神经网络 (FFN)** ：
$$
\text{FFN}(X) = \max(0, XW_1 + b_1)W_2 + b_2
$$
其中：
- $W_1$ 形状为 $(d_{\text{model}}, d_{\text{ff}})$。
- $W_2$ 形状
### 2.2.5 多头、多层注意力的 FLOPs
在 **Transformer** 中：
- **多头注意力**（Multi-Head Attention, MHA）有 $n_{\text{head}}$ 个头，每个头独立计算注意力分数。
- **多层 Transformer** 结构有 $n_{\text{layers}}$ 层。
- **批量处理** 的情况下，假设批大小为 $b$。
因此，整个 **多层、多头、批处理的注意力计算 FLOPs 为**：
$$
2 \times b \times n_{\text{layers}} \times n_{\text{head}} \times d_{\text{head}} \times t^2
$$
但由于 $d_{\text{head}} \times n_{\text{head}} = d_{\text{model}}$（模型的隐藏维度），可以简化为：
$$
2 \times b \times n_{\text{layers}} \times d_{\text{model}} \times t^2
$$
---
1. **矩阵乘法 FLOPs** ≈ $2 \cdot m \cdot n \cdot p$。
2. **单头注意力 FLOPs** ≈ $2 \cdot d_{\text{head}} \cdot t^2$。
3. **多层、多头注意力 FLOPs** ≈ $2 \cdot b \cdot n_{\text{layers}} \cdot d_{\text{model}} \cdot t^2$。
4. **计算复杂度是 $O(t^2)$，限制了长序列处理能力**。
5. 使用**Masked self-attention**（值张量乘以掩码注意力分数矩阵）所需的FLOPs与上述计算量相同。

### 2.2.6 掩码在生成阶段引发了冗余计算

![图片3](https://520activater.github.io/images/blog-posts/KV-Cache3.png)
注：启动步骤应该处理长度为1的输入序列。冗余计算的部分在浅红色和浅紫色中突出显示。浅紫色元素对应冗余计算的键和值。Generation step 2 有一个值从冗余变成了非冗余，是因为使用了掩码将要修正为0

对于使用“ What color is the sky? The sky is ”作为输入的新迭代，我们在先前生成步骤中尚未计算的唯一表示（Query,Key,Value,Attention Score)是输入序列中的最后一个词元“ is ”。更具体地说，我们需要什么来完成这一过程？
- “is”的查询向量。  
- “What”、“ color”、“ is”、“ the”、“ sky”、“?”、“The ”、“sky ”和“ is ”的键向量，用于计算"is"与这些词元的注意力分数。  
- “What”、“ color”、“ is”、“ the”、“ sky”、“?”、“The ”、“sky ”和“ is ”的值向量，用于计算"is"的Value output vector输出。

关于键和值向量，它们已经在先前的迭代中为所有词元计算过，除了“ is ”。因此，我们可以保存（即缓存Cache）并重复使用先前迭代中的键和值向量。这种优化简称为KV缓存。之后，计算“ is ”的输出表示就会变得非常简单：
1. 计算“is”的查询、键和值。  
2. 从缓存中获取“ What”、“ color”、“ is”、“ the”、“ sky”、“?”、“The ”和“sky ”的键和值向量，并将它们与我们刚刚为“is”计算的键和值向量连接起来。  
3. 使用“is”的查询和所有键来计算注意力分数（此时的注意力分数不再是矩阵，而是一个向量，表示第i个词元与第j个词元的相关性，其中i固定,j=1,2,….i）。  
4. 使用注意力分数和所有值来计算“is”的输出向量。

从我们的输入来看，只要可以使用先前词元的键和值向量（不需要查询向量），实际上就不再需要先前的词元。当我们进行KV缓存时，模型的实际输入是最后生成的词元（而不是整个序列）和KV缓存。下图是在生成阶段运行注意力层的示例。
![图片4](https://520activater.github.io/images/blog-posts/KV-Cache4.png)

## 2.3 KV缓存的线性扩展



## 2.4 KV缓存的具体操作
所有专用于文本生成的模型类（即XXXForCausalLM类）都实现了一个generate方法，该方法被用作入口点。该方法接受许多配置参数，主要用于控制词元搜索策略。KV缓存由**use_cache**布尔参数控制（默认值为True）。

`forward()` 方法就是 **transformer 的前向传播（forward pass）** 过程，但这里是 **自回归生成(autoregressive)** 任务，所以 forward 过程稍有不同.
- **训练时（没有 KV 缓存）**：
    - `input_ids` 是整个句子，一次性输入到 transformer。
    - 计算出所有 token 的 **attention** 和 **输出 logits**。
- **推理时（使用 KV 缓存）**：
    - 只输入 **最新生成的 token**（减少计算）。
    - 通过 `past_key_values` 复用过去 token 的 KV，不重复计算。
    - 生成新的 **attention KV**，返回给 `past_key_values` 供下次使用。
    - 计算 **当前 token 的 logits（预测下一个 token 的概率分布）**。

`input_ids` 是是 **输入 token 的 ID 编号**，**`ids` 是 `identifiers`（标识符）的缩写**，指的是文本经过 **tokenizer** 处理后的整数索引张量。表示当前步要输入到模型的 **新 token**（在初始推理时，是整个 prompt)。在 **生成过程中**，每次 forward 输入的`input_ids` **当前新生成的一个 token**，而不是整个上下文。
`past_key_values`存储了 **模型已经计算得到的 KV（key-value） 缓存**。
```python
past_key_values = [
    (key_0, value_0),  # 第一层 Transformer 的 KV
    (key_1, value_1),  # 第二层 Transformer 的 KV
    ...
    (key_N, value_N)   # 第 N 层 Transformer 的 KV
]
```
Transformer的每一层的KV都是一个`(key,value)`对
`key.shape = (batch_size, num_heads, sequence_length, head_dim)`
`value.shape = (batch_size, num_heads, sequence_length, head_dim)`
- **`batch_size`**：通常是 1，在推理时通常只处理一个句子。
- **`num_heads`**：多头注意力的数量，每个注意力头都有自己的 `key` 和 `value`。
- **`sequence_length`**：表示当前存储的 token 个数（即缓存的 token 长度）。
- **`head_dim`**：每个注意力头的维度（通常是 `d_model / num_heads`）。

每一层的forward方法只输入最新token，然后会生成如下新参数：
    - **`new_key`**：当前 token 计算出的 key（用于下一步注意力计算）
    - **`new_value`**：当前 token 计算出的 value（用于下一步注意力计算）
这些 `new_key` 和 `new_value` 会和 `past_key_values` 结合，形成 **更新后的 KV 缓存**，在下一次 forward 时作为 `past_key_values` 输入，以提高推理速度。

<mark class="hltr-red">注：</mark>transformer并不会在计算`key` 和 `value` 时直接生成新 token，而是通过 **前向传播（forward pass）+ 采样（sampling）** 机制来生成新的 token。
1. **前向传播 (`forward()`)**：
    - 输入 **当前 token (`input_ids`)** 和 **历史 KV 缓存 (`past_key_values`)**。
    - 计算 **新的 key-value (`new_key`, `new_value`)** 并更新 `past_key_values`。
    - 计算 **当前 token 的 logits（预测下一个 token 的概率分布）**。
2. **采样 (`sampling`)**：
    - 通过 `softmax(logits)` 计算每个 token 的概率分布。
    - 使用 **贪心搜索 (argmax) / 采样 (top-k, top-p) / 温度调节** 选择下一个 token。
    - 将生成的 **新 token** 作为 `input_ids` 输入下一次 forward。
这个循环会一直进行，直到达到最大长度或者遇到终止 token (`<eos>`)。

引用源：
[KV Cache：加速LLM推理的关键 - LexLuc - 博客园](https://www.cnblogs.com/LexLuc/p/18716439)
[【大模型推理】KV Cache原理-CSDN博客](https://blog.csdn.net/weixin_43799388/article/details/142164166)
[LLM 推理优化探微 (3) ：如何有效控制 KV 缓存的内存占用，优化推理速度？ - OSCHINA - 中文开源技术交流社区](https://my.oschina.net/IDP/blog/11046603)
[ LLM推理入门指南①：文本生成的初始化与解码阶段](https://blog.csdn.net/developeraa/article/details/138718407)
[一起理解下LLM的推理流程 - 知乎](https://zhuanlan.zhihu.com/p/18715969325)


